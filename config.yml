trainer:
  num_epochs: 50

model:
  name: "MLP"
  hidden_dims: [128, 64]
  dropout: 0.5
  activation: "ReLU"
  normalization: None #BatchNorm1d(batch必须>1)
  use_residual: False

loader:
  target_shape: [ 16, 648,256 ]
  batch_size: 1

loss:
  name: "SmoothL1Loss"

optimizer:
  lr: 0.001
  weight_decay: 0.00001